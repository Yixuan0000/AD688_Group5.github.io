[
  {
    "objectID": "data_analysis.html",
    "href": "data_analysis.html",
    "title": "Data Analysis",
    "section": "",
    "text": "import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.offline as pyo\n\n\ndata = pd.read_csv(\"lightcast_job_postings.csv\")\n\n\ncolumns_to_drop = [\n   \"ID\", \"URL\", \"ACTIVE_URLS\", \"DUPLICATES\", \"LAST_UPDATED_TIMESTAMP\",\n    \"NAICS2\", \"NAICS3\", \"NAICS4\", \"NAICS5\", \"NAICS6\",\n    \"SOC_2\", \"SOC_2_NAME\", \"SOC_3\", \"SOC_3_NAME\", \"SOC_4\", \"SOC_4_NAME\", \"SOC_5\", \"SOC_5_NAME\", \"SOC_2021_2\", \"SOC_2021_2_NAME\", \"SOC_2021_3\", \"SOC_2021_3_NAME\", \"SOC_2021_5\", \"SOC_2021_5_NAME\",\n    'NAICS_2022_2', 'NAICS_2022_2_NAME', 'NAICS_2022_3',\n       'NAICS_2022_3_NAME', 'NAICS_2022_4', 'NAICS_2022_4_NAME','NAICS_2022_5', 'NAICS_2022_5_NAME', 'SOC_2_NAME', 'SOC_3_NAME', 'SOC_4', 'SOC_4_NAME', 'SOC_5_NAME'\n]\ndata_drop = data.drop(columns=columns_to_drop)\n\n\n#pd.set_option('display.max_rows', None)\n#pd.set_option('display.max_columns', None)\n#data_drop.columns\n\n\n#Replace salary with median\nsalary_median = data_drop['SALARY'].median()\nsalary_to_median = data_drop['SALARY_TO'].median()\nsalary_from_median = data_drop['SALARY_FROM'].median()\ndata_drop['SALARY'] = data_drop['SALARY'].fillna(salary_median)\ndata_drop['SALARY_TO'] = data_drop['SALARY_TO'].fillna(salary_to_median)\ndata_drop['SALARY_FROM'] = data_drop['SALARY_FROM'].fillna(salary_from_median)\n\n\n#Replace NA Values with 0 and -1\ndata_drop['MIN_YEARS_EXPERIENCE'] = data_drop['MIN_YEARS_EXPERIENCE'].fillna(0)\ndata_drop['DURATION'] = data_drop['DURATION'].fillna(-1)\ndata_drop['MODELED_DURATION'] = data_drop['MODELED_DURATION'].fillna(-1)\n\n\n#Replace Missing Dates with Reasonable Values, and convert to date time format\ndata_drop['POSTED'] = pd.to_datetime(data['POSTED'], errors='coerce')\ndata_drop['EXPIRED'] = pd.to_datetime(data['EXPIRED'], errors='coerce')\ndata_drop['LAST_UPDATED_DATE'] = pd.to_datetime(data['LAST_UPDATED_DATE'], errors='coerce')\ndata_drop['MODELED_EXPIRED'] = pd.to_datetime(data_drop['MODELED_EXPIRED'], errors='coerce')\n\ndata_drop['EXPIRED'] = data_drop['EXPIRED'].fillna(pd.to_datetime('2100-12-31'))\ndata_drop['MODELED_EXPIRED'] = data_drop['MODELED_EXPIRED'].fillna(pd.to_datetime('2100-12-31'))\n\n\n#Handle the remaining missing values\nstring_cols = data_drop.select_dtypes(include='object').columns\ndata_drop[string_cols] = data_drop[string_cols].fillna(\"Unknown\")\n\nnumeric_cols = data_drop.select_dtypes(include=['float64', 'int64']).columns\ndata_drop[numeric_cols] = data_drop[numeric_cols].fillna(0)\n\n\ndata_drop.isna().sum()\n\nLAST_UPDATED_DATE          0\nPOSTED                     0\nEXPIRED                    0\nDURATION                   0\nSOURCE_TYPES               0\n                          ..\nLOT_V6_CAREER_AREA_NAME    0\nLIGHTCAST_SECTORS          0\nLIGHTCAST_SECTORS_NAME     0\nNAICS_2022_6               0\nNAICS_2022_6_NAME          0\nLength: 99, dtype: int64\n\n\n\n#Remove Duplicates\ndata_cleaned = data_drop.drop_duplicates(subset=[\"TITLE\", \"COMPANY\", \"LOCATION\", \"POSTED\"], keep=\"first\")\n\n\n#Data Visualization\nfig = px.bar(data_cleaned[\"NAICS_2022_6_NAME\"].value_counts(), title=\"Job Postings by Industry\")\nfig"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "AD688_Group5.github.io",
    "section": "",
    "text": "This is a Quarto website.\n\nGroup Project 1\nYixuan Yang, Arohit Talari, Chengjie Lu"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  }
]